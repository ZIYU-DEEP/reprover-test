{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb2f19bedd949e6bf4cb76702a54520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09c984a70fe491d89cab78787ffed33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/3.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2421d33354ff4149a71f192404ee6cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/3.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413d341a7f054eaabf2bfa6072496e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/837 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f432c74999de4291a3cab313cec379c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb49dfe1106493085aa59da74825079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rw [<a>Nat.gcd_comm</a>]\n",
      "\n",
      "rw [<a>Nat.gcd_comm</a>]\n",
      "rw [<a>Nat.gcd_comm</a>, <a>Nat.gcd_1</a>]\n",
      "rw [<a>Nat.gcd_comm</a>, <a>Nat.gcd_rec</a>]\n",
      "rw [<a>Nat.gcd_comm</a>, <a>Nat.gcd_gcd_self_left_right</a>]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"kaiyuy/leandojo-lean4-tacgen-byt5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"kaiyuy/leandojo-lean4-tacgen-byt5-small\")\n",
    "\n",
    "state = \"n : ℕ\\n⊢ gcd n n = n\"\n",
    "tokenized_state = tokenizer(state, return_tensors=\"pt\")\n",
    "\n",
    "# Generate a single tactic.\n",
    "tactic_ids = model.generate(tokenized_state.input_ids, max_length=1024)\n",
    "tactic = tokenizer.decode(tactic_ids[0], skip_special_tokens=True)\n",
    "print(tactic, end=\"\\n\\n\")\n",
    "\n",
    "# Generate multiple tactics via beam search.\n",
    "tactic_candidates_ids = model.generate(\n",
    "    tokenized_state.input_ids,\n",
    "    max_length=1024,\n",
    "    num_beams=4,\n",
    "    length_penalty=0.0,\n",
    "    do_sample=False,\n",
    "    num_return_sequences=4,\n",
    "    early_stopping=False,\n",
    ")\n",
    "tactic_candidates = tokenizer.batch_decode(\n",
    "    tactic_candidates_ids, skip_special_tokens=True\n",
    ")\n",
    "for tac in tactic_candidates:\n",
    "    print(tac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/hsun409/anaconda3/envs/dojo/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /nethome/hsun409/anaconda3/envs/dojo/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "benchmark.ipynb  generator  LICENSE   prover\t retrieval\n",
      "common.py\t images     mypy.ini  README.md  scripts\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['module', 'buffer_names', 'optimizer', 'param_shapes', 'frozen_param_shapes', 'shared_params', 'frozen_param_fragments', 'lr_scheduler', 'data_sampler', 'random_ltd', 'sparse_tensor_module_names', 'skipped_steps', 'global_steps', 'global_samples', 'dp_world_size', 'mp_world_size', 'ds_config', 'ds_version', 'epoch', 'global_step', 'pytorch-lightning_version', 'loops', 'callbacks', 'lr_schedulers', 'hparams_name', 'hyper_parameters'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_path = 'leandojo-pl-ckpts/generator_random.ckpt/checkpoint/mp_rank_00_model_states.pt'\n",
    "ckpt = torch.load(ckpt_path, map_location=torch.device('cpu'))\n",
    "ckpt.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from pytorch_lightning.utilities.deepspeed import (\n",
    "    convert_zero_checkpoint_to_fp32_state_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of pointing directly to the model states file, point to the parent directory.\n",
    "ckpt_path = 'leandojo-pl-ckpts/generator_random.ckpt'\n",
    "\n",
    "\n",
    "def _is_deepspeed_checkpoint(path: str):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileExistsError(f\"Checkpoint {path} does not exist.\")\n",
    "    return os.path.isdir(path) and os.path.exists(os.path.join(path, \"zero_to_fp32.py\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_is_deepspeed_checkpoint(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-01 20:26:13,920] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The default cache directory for DeepSpeed Triton autotune, /nethome/hsun409/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/hsun409/anaconda3/envs/dojo/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory 'leandojo-pl-ckpts/generator_random.ckpt/version https://git-lfs.github.com/spec/v1\noid sha256:47320987f9a49d5b00119b960f247a956773f57543982b8bfcb6da5bb3afd9ef\nsize 10' doesn't exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mTemporaryDirectory() \u001b[38;5;28;01mas\u001b[39;00m dirname:\n\u001b[1;32m      2\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dirname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlightning.cpkt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mconvert_zero_checkpoint_to_fp32_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dojo/lib/python3.10/site-packages/pytorch_lightning/utilities/deepspeed.py:81\u001b[0m, in \u001b[0;36mconvert_zero_checkpoint_to_fp32_state_dict\u001b[0;34m(checkpoint_dir, output_file, tag)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(_DEEPSPEED_AVAILABLE))\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepspeed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mzero_to_fp32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     76\u001b[0m     get_fp32_state_dict_from_zero_checkpoint,\n\u001b[1;32m     77\u001b[0m     get_model_state_file,\n\u001b[1;32m     78\u001b[0m     get_optim_files,\n\u001b[1;32m     79\u001b[0m )\n\u001b[0;32m---> 81\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mget_fp32_state_dict_from_zero_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# additional logic to ensure we keep the lightning state dict as well from rank 0.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m deepspeed_states \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmp_world_size\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     93\u001b[0m ]\n",
      "File \u001b[0;32m~/anaconda3/envs/dojo/lib/python3.10/site-packages/deepspeed/utils/zero_to_fp32.py:519\u001b[0m, in \u001b[0;36mget_fp32_state_dict_from_zero_checkpoint\u001b[0;34m(checkpoint_dir, tag, exclude_frozen_parameters)\u001b[0m\n\u001b[1;32m    516\u001b[0m ds_checkpoint_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_dir, tag)\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(ds_checkpoint_dir):\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds_checkpoint_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_fp32_state_dict_from_zero_checkpoint(ds_checkpoint_dir, exclude_frozen_parameters)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Directory 'leandojo-pl-ckpts/generator_random.ckpt/version https://git-lfs.github.com/spec/v1\noid sha256:47320987f9a49d5b00119b960f247a956773f57543982b8bfcb6da5bb3afd9ef\nsize 10' doesn't exist"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as dirname:\n",
    "    path = os.path.join(dirname, \"lightning.cpkt\")\n",
    "    convert_zero_checkpoint_to_fp32_state_dict(ckpt_path, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Required files are missing in the directory leandojo-pl-ckpts/generator_random.ckpt/checkpoint\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from pytorch_lightning.utilities.deepspeed import convert_zero_checkpoint_to_fp32_state_dict\n",
    "\n",
    "# Instead of pointing directly to the model states file, point to the parent directory.\n",
    "ckpt_path = 'leandojo-pl-ckpts/generator_random.ckpt/checkpoint'\n",
    "\n",
    "# Verify that the checkpoint directory is correctly structured and exists\n",
    "def _is_deepspeed_checkpoint(path: str):\n",
    "    required_files = [\"zero_to_fp32.py\", \"latest\"]\n",
    "    exists = all(os.path.exists(os.path.join(path, f)) for f in required_files)\n",
    "    if not exists:\n",
    "        raise FileNotFoundError(f\"Required files are missing in the directory {path}\")\n",
    "    return exists\n",
    "\n",
    "try:\n",
    "    if _is_deepspeed_checkpoint(ckpt_path):\n",
    "        with tempfile.TemporaryDirectory() as dirname:\n",
    "            path = os.path.join(dirname, \"lightning.ckpt\")\n",
    "            # Convert the Zero checkpoint to a full precision state dict for loading\n",
    "            convert_zero_checkpoint_to_fp32_state_dict(ckpt_path, path)\n",
    "            print(f\"Checkpoint converted and saved to {path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to find 'latest' file at leandojo-pl-ckpts/generator_random.ckpt/checkpoint/latest",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mTemporaryDirectory() \u001b[38;5;28;01mas\u001b[39;00m dirname:\n\u001b[1;32m      9\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dirname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlightning.cpkt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mconvert_zero_checkpoint_to_fp32_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dojo/lib/python3.10/site-packages/pytorch_lightning/utilities/deepspeed.py:81\u001b[0m, in \u001b[0;36mconvert_zero_checkpoint_to_fp32_state_dict\u001b[0;34m(checkpoint_dir, output_file, tag)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(_DEEPSPEED_AVAILABLE))\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepspeed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mzero_to_fp32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     76\u001b[0m     get_fp32_state_dict_from_zero_checkpoint,\n\u001b[1;32m     77\u001b[0m     get_model_state_file,\n\u001b[1;32m     78\u001b[0m     get_optim_files,\n\u001b[1;32m     79\u001b[0m )\n\u001b[0;32m---> 81\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mget_fp32_state_dict_from_zero_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# additional logic to ensure we keep the lightning state dict as well from rank 0.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m deepspeed_states \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmp_world_size\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     93\u001b[0m ]\n",
      "File \u001b[0;32m~/anaconda3/envs/dojo/lib/python3.10/site-packages/deepspeed/utils/zero_to_fp32.py:514\u001b[0m, in \u001b[0;36mget_fp32_state_dict_from_zero_checkpoint\u001b[0;34m(checkpoint_dir, tag, exclude_frozen_parameters)\u001b[0m\n\u001b[1;32m    512\u001b[0m             tag \u001b[38;5;241m=\u001b[39m fd\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 514\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatest\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlatest_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    516\u001b[0m ds_checkpoint_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_dir, tag)\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(ds_checkpoint_dir):\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to find 'latest' file at leandojo-pl-ckpts/generator_random.ckpt/checkpoint/latest"
     ]
    }
   ],
   "source": [
    "ckpt_path = 'leandojo-pl-ckpts/generator_random.ckpt/checkpoint'\n",
    "\n",
    "def _is_deepspeed_checkpoint(path: str):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileExistsError(f\"Checkpoint {path} does not exist.\")\n",
    "    return os.path.isdir(path) and os.path.exists(os.path.join(path, \"zero_to_fp32.py\"))\n",
    "\n",
    "with tempfile.TemporaryDirectory() as dirname:\n",
    "    path = os.path.join(dirname, \"lightning.cpkt\")\n",
    "    convert_zero_checkpoint_to_fp32_state_dict(ckpt_path, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory 'leandojo-pl-ckpts/generator_random.ckpt/version https://git-lfs.github.com/spec/v1\noid sha256:47320987f9a49d5b00119b960f247a956773f57543982b8bfcb6da5bb3afd9ef\nsize 10' doesn't exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mTemporaryDirectory() \u001b[38;5;28;01mas\u001b[39;00m dirname:\n\u001b[1;32m      9\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dirname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlightning.cpkt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mconvert_zero_checkpoint_to_fp32_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dojo/lib/python3.10/site-packages/pytorch_lightning/utilities/deepspeed.py:81\u001b[0m, in \u001b[0;36mconvert_zero_checkpoint_to_fp32_state_dict\u001b[0;34m(checkpoint_dir, output_file, tag)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(_DEEPSPEED_AVAILABLE))\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepspeed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mzero_to_fp32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     76\u001b[0m     get_fp32_state_dict_from_zero_checkpoint,\n\u001b[1;32m     77\u001b[0m     get_model_state_file,\n\u001b[1;32m     78\u001b[0m     get_optim_files,\n\u001b[1;32m     79\u001b[0m )\n\u001b[0;32m---> 81\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mget_fp32_state_dict_from_zero_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# additional logic to ensure we keep the lightning state dict as well from rank 0.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m deepspeed_states \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmp_world_size\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     93\u001b[0m ]\n",
      "File \u001b[0;32m~/anaconda3/envs/dojo/lib/python3.10/site-packages/deepspeed/utils/zero_to_fp32.py:519\u001b[0m, in \u001b[0;36mget_fp32_state_dict_from_zero_checkpoint\u001b[0;34m(checkpoint_dir, tag, exclude_frozen_parameters)\u001b[0m\n\u001b[1;32m    516\u001b[0m ds_checkpoint_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_dir, tag)\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(ds_checkpoint_dir):\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds_checkpoint_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_fp32_state_dict_from_zero_checkpoint(ds_checkpoint_dir, exclude_frozen_parameters)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Directory 'leandojo-pl-ckpts/generator_random.ckpt/version https://git-lfs.github.com/spec/v1\noid sha256:47320987f9a49d5b00119b960f247a956773f57543982b8bfcb6da5bb3afd9ef\nsize 10' doesn't exist"
     ]
    }
   ],
   "source": [
    "ckpt_path = 'leandojo-pl-ckpts/generator_random.ckpt'\n",
    "\n",
    "def _is_deepspeed_checkpoint(path: str):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileExistsError(f\"Checkpoint {path} does not exist.\")\n",
    "    return os.path.isdir(path) and os.path.exists(os.path.join(path, \"zero_to_fp32.py\"))\n",
    "\n",
    "with tempfile.TemporaryDirectory() as dirname:\n",
    "    path = os.path.join(dirname, \"lightning.cpkt\")\n",
    "    convert_zero_checkpoint_to_fp32_state_dict(ckpt_path, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing zero checkpoint 'leandojo-pl-ckpts/generator_random.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage 2, world_size: 1\n",
      "Parsing checkpoint created by deepspeed==0.14.0\n",
      "Reconstructed fp32 state dict with 170 params 299637760 elements\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory '<function ds_checkpoint_dir at 0x7fe6191a3640>' doesn't exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mTemporaryDirectory() \u001b[38;5;28;01mas\u001b[39;00m dirname:\n\u001b[1;32m     10\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dirname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlightning.cpkt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mconvert_zero_checkpoint_to_fp32_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dojo/lib/python3.10/site-packages/pytorch_lightning/utilities/deepspeed.py:94\u001b[0m, in \u001b[0;36mconvert_zero_checkpoint_to_fp32_state_dict\u001b[0;34m(checkpoint_dir, output_file, tag)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# additional logic to ensure we keep the lightning state dict as well from rank 0.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m deepspeed_states \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmp_world_size\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     93\u001b[0m ]\n\u001b[0;32m---> 94\u001b[0m checkpoint_dir \u001b[38;5;241m=\u001b[39m \u001b[43mds_checkpoint_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m optim_files \u001b[38;5;241m=\u001b[39m get_optim_files(checkpoint_dir)\n\u001b[1;32m     96\u001b[0m optim_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(optim_files[\u001b[38;5;241m0\u001b[39m], map_location\u001b[38;5;241m=\u001b[39mCPU_DEVICE)\n",
      "File \u001b[0;32m~/anaconda3/envs/dojo/lib/python3.10/site-packages/pytorch_lightning/utilities/deepspeed.py:41\u001b[0m, in \u001b[0;36mds_checkpoint_dir\u001b[0;34m(checkpoint_dir, tag)\u001b[0m\n\u001b[1;32m     38\u001b[0m directory \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_dir, tag)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(directory):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds_checkpoint_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m directory\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Directory '<function ds_checkpoint_dir at 0x7fe6191a3640>' doesn't exist"
     ]
    }
   ],
   "source": [
    "ckpt_path = 'leandojo-pl-ckpts/generator_random.ckpt'\n",
    "tag = 'checkpoint'\n",
    "\n",
    "def _is_deepspeed_checkpoint(path: str):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileExistsError(f\"Checkpoint {path} does not exist.\")\n",
    "    return os.path.isdir(path) and os.path.exists(os.path.join(path, \"zero_to_fp32.py\"))\n",
    "\n",
    "with tempfile.TemporaryDirectory() as dirname:\n",
    "    path = os.path.join(dirname, \"lightning.cpkt\")\n",
    "    convert_zero_checkpoint_to_fp32_state_dict(ckpt_path, path, tag=tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to find 'latest' file at leandojo-pl-ckpts/generator_random.ckpt/checkpoint/latest",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mTemporaryDirectory() \u001b[38;5;28;01mas\u001b[39;00m dirname:\n\u001b[1;32m      9\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dirname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlightning.cpkt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mconvert_zero_checkpoint_to_fp32_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dojo/lib/python3.10/site-packages/pytorch_lightning/utilities/deepspeed.py:81\u001b[0m, in \u001b[0;36mconvert_zero_checkpoint_to_fp32_state_dict\u001b[0;34m(checkpoint_dir, output_file, tag)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(_DEEPSPEED_AVAILABLE))\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepspeed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mzero_to_fp32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     76\u001b[0m     get_fp32_state_dict_from_zero_checkpoint,\n\u001b[1;32m     77\u001b[0m     get_model_state_file,\n\u001b[1;32m     78\u001b[0m     get_optim_files,\n\u001b[1;32m     79\u001b[0m )\n\u001b[0;32m---> 81\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mget_fp32_state_dict_from_zero_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# additional logic to ensure we keep the lightning state dict as well from rank 0.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m deepspeed_states \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmp_world_size\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     93\u001b[0m ]\n",
      "File \u001b[0;32m~/anaconda3/envs/dojo/lib/python3.10/site-packages/deepspeed/utils/zero_to_fp32.py:514\u001b[0m, in \u001b[0;36mget_fp32_state_dict_from_zero_checkpoint\u001b[0;34m(checkpoint_dir, tag, exclude_frozen_parameters)\u001b[0m\n\u001b[1;32m    512\u001b[0m             tag \u001b[38;5;241m=\u001b[39m fd\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 514\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatest\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlatest_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    516\u001b[0m ds_checkpoint_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_dir, tag)\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(ds_checkpoint_dir):\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to find 'latest' file at leandojo-pl-ckpts/generator_random.ckpt/checkpoint/latest"
     ]
    }
   ],
   "source": [
    "ckpt_path = 'leandojo-pl-ckpts/generator_random.ckpt/checkpoint'\n",
    "\n",
    "def _is_deepspeed_checkpoint(path: str):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileExistsError(f\"Checkpoint {path} does not exist.\")\n",
    "    return os.path.isdir(path) and os.path.exists(os.path.join(path, \"zero_to_fp32.py\"))\n",
    "\n",
    "with tempfile.TemporaryDirectory() as dirname:\n",
    "    path = os.path.join(dirname, \"lightning.cpkt\")\n",
    "    convert_zero_checkpoint_to_fp32_state_dict(ckpt_path, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing zero checkpoint 'leandojo-pl-ckpts/generator_random.ckpt/checkpoint/'\n",
      "Detected checkpoint of type zero stage 2, world_size: 1\n",
      "Parsing checkpoint created by deepspeed==0.14.0\n",
      "Reconstructed fp32 state dict with 170 params 299637760 elements\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to find 'latest' file at leandojo-pl-ckpts/generator_random.ckpt/checkpoint/latest",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mTemporaryDirectory() \u001b[38;5;28;01mas\u001b[39;00m dirname:\n\u001b[1;32m      9\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dirname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlightning.cpkt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mconvert_zero_checkpoint_to_fp32_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dojo/lib/python3.10/site-packages/pytorch_lightning/utilities/deepspeed.py:94\u001b[0m, in \u001b[0;36mconvert_zero_checkpoint_to_fp32_state_dict\u001b[0;34m(checkpoint_dir, output_file, tag)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# additional logic to ensure we keep the lightning state dict as well from rank 0.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m deepspeed_states \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmp_world_size\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     93\u001b[0m ]\n\u001b[0;32m---> 94\u001b[0m checkpoint_dir \u001b[38;5;241m=\u001b[39m \u001b[43mds_checkpoint_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m optim_files \u001b[38;5;241m=\u001b[39m get_optim_files(checkpoint_dir)\n\u001b[1;32m     96\u001b[0m optim_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(optim_files[\u001b[38;5;241m0\u001b[39m], map_location\u001b[38;5;241m=\u001b[39mCPU_DEVICE)\n",
      "File \u001b[0;32m~/anaconda3/envs/dojo/lib/python3.10/site-packages/pytorch_lightning/utilities/deepspeed.py:36\u001b[0m, in \u001b[0;36mds_checkpoint_dir\u001b[0;34m(checkpoint_dir, tag)\u001b[0m\n\u001b[1;32m     34\u001b[0m             tag \u001b[38;5;241m=\u001b[39m fd\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatest\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlatest_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m directory \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_dir, tag)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(directory):\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to find 'latest' file at leandojo-pl-ckpts/generator_random.ckpt/checkpoint/latest"
     ]
    }
   ],
   "source": [
    "ckpt_path = 'leandojo-pl-ckpts/generator_random.ckpt/checkpoint'\n",
    "\n",
    "def _is_deepspeed_checkpoint(path: str):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileExistsError(f\"Checkpoint {path} does not exist.\")\n",
    "    return os.path.isdir(path) and os.path.exists(os.path.join(path, \"zero_to_fp32.py\"))\n",
    "\n",
    "with tempfile.TemporaryDirectory() as dirname:\n",
    "    path = os.path.join(dirname, \"lightning.cpkt\")\n",
    "    convert_zero_checkpoint_to_fp32_state_dict(ckpt_path, path, tag='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing zero checkpoint 'leandojo-pl-ckpts/generator_random.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage 2, world_size: 1\n",
      "Parsing checkpoint created by deepspeed==0.14.0\n",
      "Reconstructed fp32 state dict with 170 params 299637760 elements\n",
      "Saving fp32 state dict to /tmp/tmpy2ayeo8r/lightning.cpkt\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = 'leandojo-pl-ckpts/generator_random.ckpt'\n",
    "\n",
    "def _is_deepspeed_checkpoint(path: str):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileExistsError(f\"Checkpoint {path} does not exist.\")\n",
    "    return os.path.isdir(path) and os.path.exists(os.path.join(path, \"zero_to_fp32.py\"))\n",
    "\n",
    "with tempfile.TemporaryDirectory() as dirname:\n",
    "    path = os.path.join(dirname, \"lightning.cpkt\")\n",
    "    convert_zero_checkpoint_to_fp32_state_dict(ckpt_path, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dojo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
